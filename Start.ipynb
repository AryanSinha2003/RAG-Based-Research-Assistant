{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8270a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"/Users/aryansinha/Desktop/Lallma Project/Diabetic_Retinopathy_Image_Classification_Using_Machine_Learning_and_Local_Binary_Patterns_Features copy.pdf\")    # or use UnstructuredPDFLoader\n",
    "docs = loader.load()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789fbeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 28\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e8a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embed_model = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefd1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings: 28\n"
     ]
    }
   ],
   "source": [
    "docs_embeddings = embed_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "print(f\"Total embeddings: {len(docs_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4d7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/_5s1cj2s5bj3644p722_9pfc0000gn/T/ipykernel_55972/2056404052.py:3: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(chunks, embed_model, persist_directory=\"db\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5530d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ac83209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/_5s1cj2s5bj3644p722_9pfc0000gn/T/ipykernel_55972/3997974038.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.7,\n",
    "    callback_manager=None\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122e20f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/_5s1cj2s5bj3644p722_9pfc0000gn/T/ipykernel_55972/2246608304.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = summarizer.run(docs)  # docs = list of full Document pages or large chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a concise summary:\n",
      "\n",
      "A study evaluated the performance of six machine learning algorithms (Random Forest, Adaptive Boosting, K-Nearest Neighbor, Gaussian Naive Bayes, Support Vector Machine, and Quadratic Discriminant Analysis) in classifying diabetic retinopathy images into three categories. The results showed that Random Forest achieved the highest classification accuracy (0.912-0.94), particularly for distinguishing between non-diabetic retinopathy and severe retinopathy. The study highlights the effectiveness of Local Binary Patterns (LBP) features in improving accuracy with machine learning models, suggesting potential future research integrating these features with deep learning techniques.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summarizer = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "summary = summarizer.run(docs)  # docs = list of full Document pages or large chunks\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef736ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,   # to get source chunks\n",
    "    chain_type=\"stuff\",             # or \"map_reduce\" if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6048c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is the highest accuracy achieved in the study?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e66f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/_5s1cj2s5bj3644p722_9pfc0000gn/T/ipykernel_55972/499111952.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": user_question})\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": user_question})\n",
    "answer = result[\"result\"]\n",
    "sources = result[\"source_documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e77881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text doesn't explicitly state the question being asked, but based on the provided information, it appears to be asking for the highest accuracy achieved in the study. According to the text, the RF classifier achieves an accuracy of 0.94 when classifying between noDR and SV.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd0eaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not create share link. Missing file: /Users/aryansinha/.cache/huggingface/gradio/frpc/frpc_darwin_amd64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_darwin_amd64\n",
      "2. Rename the downloaded file to: frpc_darwin_amd64_v0.3\n",
      "3. Move the file to this location: /Users/aryansinha/.cache/huggingface/gradio/frpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def answer_pdf(pdf_file, question):\n",
    "    # Handle file upload properly\n",
    "    if pdf_file is None:\n",
    "        return \"Please upload a PDF file\"\n",
    "    \n",
    "    try:\n",
    "        # Create a temporary file and write the PDF content\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n",
    "            temp_file.write(pdf_file)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        # Process the uploaded file using the temporary file path\n",
    "        loader = PyMuPDFLoader(temp_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Create text chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = Chroma.from_documents(chunks, embed_model, persist_directory=\"db\")\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "        \n",
    "        # Create QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type=\"stuff\"\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        result = qa_chain({\"query\": question})\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "        \n",
    "        return result[\"result\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=answer_pdf,\n",
    "    inputs=[\n",
    "        gr.File(label=\"PDF Document\", type=\"binary\"),\n",
    "        gr.Textbox(label=\"Question\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Automated Research Assistant\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True,pwa=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
