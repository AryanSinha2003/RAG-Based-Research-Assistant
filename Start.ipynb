{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"Diabetic_Retinopathy_Image_Classification_Using_Machine_Learning_and_Local_Binary_Patterns_Features copy.pdf\")    # or use UnstructuredPDFLoader\n",
    "docs = loader.load()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embed_model = OllamaEmbeddings(model=\"llama3-chatqa:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_embeddings = embed_model.embed_documents([chunk.page_content for chunk in chunks])\n",
    "print(f\"Total embeddings: {len(docs_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(chunks, embed_model, persist_directory=\"db\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac83209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"llama3-chatqa:8b\",\n",
    "    temperature=0.5,\n",
    "    callback_manager=None\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summarizer = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "summary = summarizer.run(docs)  # docs = list of full Document pages or large chunks\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef736ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,   # to get source chunks\n",
    "    chain_type=\"stuff\",             # or \"map_reduce\" if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6048c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is the highest accuracy achieved in the study?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e66f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": user_question})\n",
    "answer = result[\"result\"]\n",
    "sources = result[\"source_documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0eaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def answer_pdf(pdf_file, question):\n",
    "    # Handle file upload properly\n",
    "    if pdf_file is None:\n",
    "        return \"Please upload a PDF file\"\n",
    "    \n",
    "    try:\n",
    "        # Create a temporary file and write the PDF content\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:\n",
    "            temp_file.write(pdf_file)\n",
    "            temp_path = temp_file.name\n",
    "        \n",
    "        # Process the uploaded file using the temporary file path\n",
    "        loader = PyMuPDFLoader(temp_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Create text chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = Chroma.from_documents(chunks, embed_model, persist_directory=\"db\")\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "        \n",
    "        # Create QA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type=\"stuff\"\n",
    "        )\n",
    "        \n",
    "        # Get answer\n",
    "        result = qa_chain({\"query\": question})\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_path)\n",
    "        \n",
    "        return result[\"result\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=answer_pdf,\n",
    "    inputs=[\n",
    "        gr.File(label=\"PDF Document\", type=\"binary\"),\n",
    "        gr.Textbox(label=\"Question\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Automated Research Assistant\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True,pwa=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
